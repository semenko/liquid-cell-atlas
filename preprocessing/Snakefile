## This is the Liquid Cell Atlas pipeline from raw reads to methylation data, including extensive QC
# 
# Author: Nick Semenkovich <semenko@alum.mit.edu> https://nick.semenkovich.com
# Source: https://github.com/semenko/liquid-cell-atlas
#
# Note: There are modular ways to approach things with snakefiles â€” here, I explicitly prioritized
# simplicity and having one large file that runs the same analyses on all inputs. For example, there's
# no modular .yaml config -- we just queue this up as a cron job.
#
# If you're looking for different approaches, you can explore:
# Snakepipes WGBS pipeline: https://snakepipes.readthedocs.io/en/latest/content/workflows/WGBS.html
# Snakemake Wrappers: https://github.com/snakemake/snakemake-wrappers/

import glob
import re

import pandas as pd
from snakemake.remote.HTTP import RemoteProvider as HTTPRemoteProvider

CONFIG_FILE_PATH = "/home/nsemenkovich/liquid-cell-atlas/data/internal"
DATA_PATH = "/logo2/lca_data"

# For .fa files and indexes (.c2t from bwa index)
REFERENCE_PATH = "/logo2/reference_data"

data_config_files = glob.glob(f"{CONFIG_FILE_PATH}/*.csv")

# for file in data_config_files:
SAMPLE_DF = pd.read_table(f"{CONFIG_FILE_PATH}/melanoma.csv", delimiter=",").set_index("path_R1", drop=False)
SAMPLES = SAMPLE_DF['path_R1'].tolist()
SAMPLES = ['YUVASI-09-1042_R1.fastq.gz', 'YUZAT-16-3532_R1.fastq.gz', 'YURADIO-14-3253_R1.fastq.gz', 'YUBAIX-15-3422_R1.fastq.gz']

#samplesData = pandas.read_csv(input_table, header=None).loc[:, 0].tolist()
#samples = [re.findall("[^/]+\.", sample)[0][:-1] for sample in samplesData]  # overly complicated regex

# Snakemake is a little funny.
# The first function here just defines what *output* we'll see when the whole pipeline is done.
rule all:
    input:
        expand("/logo2/lca_output/{sample}.txt", sample=SAMPLES)


### fastp
# Here we do trimming, but almost no quality filtering (leaving that for downstream)
# We adapter trim and do poly-g filtering, but leave quality filtering / polyN filtering for later.
rule fastp:
    input:
        sample=["reads/pe/{sample}.1.fastq", "reads/pe/{sample}.2.fastq"]
    output:
        pipe()
        trimmed=["trimmed/pe/{sample}.1.fastq", "trimmed/pe/{sample}.2.fastq"],
        discarded = ""  # Length < 10
        fastp_json = "report/pe/{sample}.json"
        fastp_html = "report/pe/{sample}.html"
    log:
        "logs/fastp/pe/{sample}.log"
    params:
        # Inspired to do poly-g trimming by the NEB EM-seq pipeline:
        # https://github.com/nebiolabs/EM-seq/blob/master/em-seq.nf
        # TODO: make this selective for NextSeq-only runs?
        # e.g.:
        #    inst_name=$(zcat -f '!{fq_set.insert_read1}' | head -n 1 | cut -f 1 -d ':' | sed 's/^@//')
        #    fastq_barcode=$(zcat -f '!{fq_set.insert_read1}' | head -n 1 | sed -r 's/.*://')
        #    if [[ "${inst_name:0:2}" == 'A0' ]] || [[ "${inst_name:0:2}" == 'NS' ]] || \
        #       [[ "${inst_name:0:2}" == 'NB' ]] || [[ "${inst_name:0:2}" == 'VH' ]] ; then
        trim_poly_g_flag = "--trim_poly_g"

        # Trim specific to BS vs EM-seq input type
        trim_5prime = "10"
        trim_3prime = "10"

        # TruSeq adapters (we could autodetect, but not needed currently).
        adapter_r1 = "AGATCGGAAGAGCACACGTCTGAACTCCAGTCA"
        adapter_r2 = "AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT"
    shell:
        """
        fastp --in1 {input.sample_pe1} --in2 {input.sample_pe2} \
        --trim_front1 {params.trim_length} --trim_tail1 {params.trim_length} \
        --trim_front2 {params.trim_length} --trim_tail2 {params.trim_length} \
        --adapter_sequence {params.adapter_r1} --adapter_sequence_r2 {params.adapter_r2} \
        --length_required 10 --disable_quality_filtering --overrepresentation_analysis \
        {trim_polyg} \
        --json "{output.fastp_json}" --html "{output.fastp_html}" \
        --thread {snakemake.threads} \
        2> fastp.stderr
        """"


### Build GRCh38 reference
# This should run once to generate a standard reference genome, plus a contamination/repair bed for the U2AF1 issue:
# https://genomeref.blogspot.com/2021/07/one-of-these-things-doest-belong.html
# NOTE: This does not include patches (e.g. p14), even though the URL suggests otherwise.
# This is: GRCh38, with decoys and hs38d1, but no ALT and no patch. For a better understanding, read:
# https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/405/GCF_000001405.40_GRCh38.p14/GRCh38_major_release_seqs_for_alignment_pipelines/README_analysis_sets.txt
# This is similar to the Verily standard (GRCh38_Verily_v1)
HTTP = HTTPRemoteProvider()

rule build_reference_genome:
    input:
        # Not really ftp -- it's HTTPS/TLS
        HTTP.remote("ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/405/GCF_000001405.40_GRCh38.p14/GRCh38_major_release_seqs_for_alignment_pipelines/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna.gz", keep_local=True)
    output:
        "{DATA_PATH}/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna.gz"
    shell:

    run:
        outputName = os.path.basename(input[0])
        shell("mv {input} {outputName}")




### bwa_meth indices
# This should run once to generate bwa-meth indices for bwa mem
rule bwa_meth_indices:
    input:
    output:
        "ref.fa.bwameth.c2t"
        "ref.fa.bwameth.c2t.amb"
        "ref.fa.bwameth.c2t.ann"
        "ref.fa.bwameth.c2t.bwt"
        "ref.fa.bwameth.c2t.pac"
        "ref.fa.bwameth.c2t.sa"
    shell:
        "bwameth.py index {REFERENCE}"

### bwa_meth
rule bwa_meth:
    input:
        pipe(".fastp.fastq")
    output:
        pipe(".bwameth.sam")
    shell:
        "bwameth.py -p -t !{task.cpus} --read-group "@RG\\tID:${fastq_barcode}\\tSM:!{fq_set.library}" --reference !{genome} /dev/stdin \
                 2>  "!{fq_set.library}_${fastq_barcode}!{fq_set.flowcell}_!{fq_set.lane}_!{fq_set.tile}.log.bwamem" "


### mark nonconverted reads
# This is a clever little program from NEB that marks non-converted reads:
# https://github.com/nebiolabs/mark-nonconverted-reads
# This sets XX:Z:UC (the X? Y? and Z? fields of SAM/BAM are user-reserved). We do *not* set the Vendor Failed bit.
# Note that  MethylDackel can independently identify poor conversion (set minConversionEfficiency).
rule mark_nonconverted:
    input:
        pipe(".bwameth.sam")
    output:
        pipe(".bwameth.nonconv.sam")
    shell:
        "mark-nonconverted-reads.py 2> "!{fq_set.library}_${fastq_barcode}_!{fq_set.flowcell}_!{fq_set.lane}_!{fq_set.tile}.nonconverted.tsv" \""

### sort
# Coordinate sort our input sam, and stream as a sam (no bam yet)
rule samtools_sort:
    input:
        pipe(".bwameth.nonconv.sam")
    output:
        pipe(".sorted.sam")
    shell:
        "samtools sort --output-fmt SAM --threads {snakemake.threads} {input} {output}"

### fixmate
# We need to ad ms and MC tags for markdup to run next
rule samtools_fixmate:
    input:
        pipe(".sorted.sam")
    output:
        pipe(".fixmate.bam")
    shell:
        # -u uncompressed output
        # -m add mate score tag
        "samtools fixmate -u -m --output-fmt SAM --threads {snakemake.threads} {input} {output}"

### markdup
# Mark duplicates from a .bam file
rule samtools_markdup:
    input:
        pipe(".fixmate.sam")
    output:
        "{sample}.bam"
    shell:
        # TODO: Set mark supplementary alignments of duplicates as duplicates? (-S)
        # -f: Write stats to file
        # 
        # Output with compress level 1 (lowest)
        # ? Write index here
        # "-##idx##indexname" 
        "samtools markdup  -f {stats.file} --output-fmt BAM --output-fmt-option level=1 --threads {snakemake.threads} {input} {output} "

rule samtools_flagstat:
    input:
        "{sample}.bam"
    output:
        "{sample}.txt"
    shell:
        "samtools flagstat"

### MethylDackel mbias
# Make per-sample mbias plots
rule methyldackel_mbias_plots:
    input:
    output:
    shell:

### DeepTools bias plots:
# ? Are these useful
rule deeptools_bias_plots:
    input:
    output:
    shell:

### wgbs_tools
# Generate pat/beta file formats
rule wgbs_tools_prep:
    input:
    output:
    shell:

### multiqc
# Aggregate all our pretty data
rule multiqc:
    input:
    output:
    shell:


# Get the file sizes and add to a file
rule get_size:
    input:
        "/logo2/lca_data/melanoma/{sample}"
    output:
        "/logo2/lca_output/{sample}.txt"
    shell:
        "ls -lha {input} > {output}"
