## This is the Liquid Cell Atlas pipeline from raw reads to methylation data, including extensive QC
# 
# Author: Nick Semenkovich <semenko@alum.mit.edu> https://nick.semenkovich.com
# Source: https://github.com/semenko/liquid-cell-atlas
#
# Note: There are modular ways to approach things with snakefiles — here, I explicitly prioritized
# simplicity and having one large file that runs the same analyses on all inputs. For example, there's
# no modular .yaml config.
#
# If you're looking for different approaches, you can explore:
# Snakepipes WGBS pipeline: https://snakepipes.readthedocs.io/en/latest/content/workflows/WGBS.html
# Snakemake Wrappers: https://github.com/snakemake/snakemake-wrappers/

import pathlib
import glob
import re
import sys

import pandas as pd
from snakemake.remote.HTTP import RemoteProvider as HTTPRemoteProvider

DATA_PATH = "../data"

# Loop over the experiment definition files (e.g. melanoma.csv, crc.csv)
data_config_files = glob.glob(f"{DATA_PATH}/*.csv")

# Uncomment for test runs
data_config_files = glob.glob(f"{DATA_PATH}/melanoma.csv")

# Store a big table of all our experiment .csvs
all_experiments_df = pd.DataFrame()

### Parse the sample .csv definitions
for filename in data_config_files:
    # The experiment "name", which is also the path prefix, e.g. crc / melanoma
    experiment = pathlib.Path(filename).stem

    print(f"Parsing experiment: {experiment}", file=sys.stderr)

    # Load the .csv of our sample data
    experiment_df = pd.read_table(filename, delimiter=",").set_index("sample_id", drop=False)

    experiment_df['experiment'] = experiment

    # If these parameters aren't defined, drop the entry
    original_length = len(experiment_df)
    experiment_df.dropna(subset=['experiment', 'sample_id', 'path_R1', 'path_R2', 'method', 'md5sum_R1', 'md5sum_R2'], inplace=True)    
    new_length = len(experiment_df)
    
    if original_length != new_length:
      print(f"\tWarning: Dropped {original_length-new_length} rows from experiment '{experiment}' that were missing required values.", file=sys.stderr)

    # If the subsampled flag is enabled, that means we're *operating* on subsampled data
    # (We're not making it -- it's made on regular / full runs.)
    if "subsampled" in config:
        # So we'll append the _subsampled suffix to experiments
        experiment_df['experiment'] = experiment + "_subsampled"

        # Note that subsampled data all have the same path data
        experiment_df = experiment_df.assign(path_R1='R1.fastq.gz', path_R2='R2.fastq.gz')
        # For subsampled reads, we've touch()'d the md5 validation file, so the pipeline will skip it.
        experiment_df = experiment_df.assign(md5sum_R1=None, md5sum_R2=None)

    # Append to one mega dataframe of all experiments
    all_experiments_df = pd.concat([all_experiments_df, experiment_df])


# This is a list of dicts, where each entry is a single sample like:
# {'experiment': 'melanoma', 'sample_id': 'Melanoma_B08_YURILES-14-3250',
#   'path_R1': 'YURILES-14-3250_R1.fastq.gz', 'path_R2': 'YURILES-14-3250_R2.fastq.gz',
#   'method': 'emseq', 'md5sum_R1': '4be250532…', 'md5sum_R2': '783e91bd2…'}
ALL_SAMPLES_LIST = []

## Select just the columns we want.
# orient=records returns a list of dicts
all_experiments_data = all_experiments_df[['experiment', 'sample_id', 'path_R1', 'path_R2', 'method', 'md5sum_R1', 'md5sum_R2']].to_dict(orient="records")

print(f"Parsed {len(all_experiments_data)} total sample files.", file=sys.stderr)

# For building a sample list for Snakemake
ALL_SAMPLES_LIST.extend(all_experiments_data)


###############
# Snakemake is a little funny.
# The first function here just defines what *output* we expect when the whole pipeline is done.
###############
rule all:
    input:
        # If you do NOT pass the subsampled flag, then we'll generate subsampled reads (from the full .fastq)
        [f"{DATA_PATH}/{sample['experiment']}_subsampled/{sample['sample_id']}/raw/R1.fastq.gz" for sample in ALL_SAMPLES_LIST] if "subsampled" not in config else [],
        [f"{DATA_PATH}/{sample['experiment']}_subsampled/{sample['sample_id']}/raw/R2.fastq.gz" for sample in ALL_SAMPLES_LIST] if "subsampled" not in config else [],
        # Multiqc is our last step
        [f"{DATA_PATH}/{sample['experiment']}/{sample['sample_id']}/multiqc/multiqc_report.html" for sample in ALL_SAMPLES_LIST],

###############
### Download & Build References
# These only run once.
###############

### Build GRCh38 reference
# Download our standard reference genome
# NOTE: This does not include patches (e.g. p14), even though the URL suggests otherwise.
# This is: GRCh38, with decoys and hs38d1, but no ALT and no patch. For a better understanding, read:
# https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/405/GCF_000001405.40_GRCh38.p14/GRCh38_major_release_seqs_for_alignment_pipelines/README_analysis_sets.txt

HTTPS = HTTPRemoteProvider()
rule get_reference_genome:
    input:
        # Not really ftp -- it's HTTPS/TLS
        reference_genome = HTTPS.remote("ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/405/GCF_000001405.40_GRCh38.p14/GRCh38_major_release_seqs_for_alignment_pipelines/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna.gz", keep_local=True)
    output:
        reference_genome = "{DATA_PATH}/reference/GRCh38.fna"
    run:
        shell("gunzip -c {input.reference_genome} > {output.reference_genome}")

# Apply both the NCBI U2AF1 fix and the Encode DAC Exclusion List
# U2AF1 masking file details: https://genomeref.blogspot.com/2021/07/one-of-these-things-doest-belong.html
# Encode DAC Exclusion List details: https://www.encodeproject.org/annotations/ENCSR636HFF/
rule mask_reference_fasta:
    # TODO: Consider a more modular env definition? (one per rule / rule group?)
    conda: "envs/env.yaml"
    input:
        u1af1_exclusion = HTTPS.remote("ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/405/GCF_000001405.40_GRCh38.p14/GRCh38_major_release_seqs_for_alignment_pipelines/GCA_000001405.15_GRCh38_GRC_exclusions.bed", keep_local=True),
        dac_exclusion = HTTPS.remote("encode-public.s3.amazonaws.com/2020/05/05/bc5dcc02-eafb-4471-aba0-4ebc7ee8c3e6/ENCFF356LFX.bed.gz", keep_local=True),
        reference_genome = ancient(rules.get_reference_genome.output.reference_genome)  # Ancient ignores the mtime: if this file exists, assume we ran.
    output:
        masked_reference_genome = "{DATA_PATH}/reference/GRCh38-DAC-U2AF1.fna"
    log:
        maskfasta = "{DATA_PATH}/reference/maskfasta.log.txt"
    shell:
        """
        gunzip -c {input.dac_exclusion} > {DATA_PATH}/reference/DAC-Exclusion-List-ENCFF356LFX.bed
        cp {input.u1af1_exclusion} {DATA_PATH}/reference/U2AF1-GRCh38-Exclusion.bed
        # maskfasta doesn't work on stdin/stdout, or with gzipped files :/
        # Mask for DAC
        bedtools maskfasta -fi {input.reference_genome} -fo {DATA_PATH}/reference/GRCh38-DAC.fna -bed {DATA_PATH}/reference/DAC-Exclusion-List-ENCFF356LFX.bed >>{log.maskfasta} 2>&1
        # Mask for U2AF1
        bedtools maskfasta -fi {DATA_PATH}/reference/GRCh38-DAC.fna -fo {output.masked_reference_genome} -bed {DATA_PATH}/reference/U2AF1-GRCh38-Exclusion.bed >>{log.maskfasta} 2>&1
        """

### bwa meth index
# This should run once to generate indices for bwa mem (through bwa-meth)
rule bwa_meth_index:
    conda: "envs/env.yaml"
    input:
        masked_reference_genome = ancient(rules.mask_reference_fasta.output)
    output:
        "{DATA_PATH}/reference/GRCh38-DAC-U2AF1.fna.bwameth.c2t",
        "{DATA_PATH}/reference/GRCh38-DAC-U2AF1.fna.bwameth.c2t.amb",
        "{DATA_PATH}/reference/GRCh38-DAC-U2AF1.fna.bwameth.c2t.ann",
        "{DATA_PATH}/reference/GRCh38-DAC-U2AF1.fna.bwameth.c2t.bwt",
        "{DATA_PATH}/reference/GRCh38-DAC-U2AF1.fna.bwameth.c2t.pac",
        "{DATA_PATH}/reference/GRCh38-DAC-U2AF1.fna.bwameth.c2t.sa",
    log:
        index = "{DATA_PATH}/reference/bwameth_index.log.txt"
    shell:
        "bwameth.py index {input.masked_reference_genome} >{log.index} 2>&1"

###############


###############
### Core Pipeline Steps
###############

### md5
# Validate checksums
rule md5sum:
    input:
        r1_file = lambda wildcards: "{DATA_PATH}/{experiment}/{sample}/raw/" + all_experiments_df.loc[wildcards.sample, 'path_R1'],
        r2_file = lambda wildcards: "{DATA_PATH}/{experiment}/{sample}/raw/" + all_experiments_df.loc[wildcards.sample, 'path_R2'],
    output:
        checksums = "{DATA_PATH}/{experiment}/{sample}/logs/md5.known.txt",
        validated = "{DATA_PATH}/{experiment}/{sample}/logs/md5.validated.txt"
    params:
        r1_checksum = lambda wildcards: all_experiments_df.loc[wildcards.sample, 'md5sum_R1'],
        r2_checksum = lambda wildcards: all_experiments_df.loc[wildcards.sample, 'md5sum_R2']
    shell:
        """
        echo -e "{params.r1_checksum}\t{input.r1_file}\n{params.r2_checksum}\t{input.r2_file}" > {output.checksums}
        md5sum --check {output.checksums} > {output.validated}
        """

### subsample
# This rule only executes if you do NOT set the subsampled flag -- it generates parallel experiment 
# directories consisting of subsampled reads to perform rapid analyses.
rule seqtk_subsample:
    input:
        md5_sums = rules.md5sum.output.validated,
        r1_file = rules.md5sum.input.r1_file,
        r2_file = rules.md5sum.input.r2_file,
    output:
        # Actual subsampled reads
        r1_file_ss = "{DATA_PATH}/{experiment}_subsampled/{sample}/raw/R1.fastq.gz",
        r2_file_ss = "{DATA_PATH}/{experiment}_subsampled/{sample}/raw/R2.fastq.gz",
        # We touch these to skip md5 validation on subsampled data
        md5_known = touch("{DATA_PATH}/{experiment}_subsampled/{sample}/logs/md5.known.txt"),
        md5_validated = touch("{DATA_PATH}/{experiment}_subsampled/{sample}/logs/md5.validated.txt"),
    params:
        seed = 4242, # seed required for stable pairs
        reads = 5000000 # 5 million-per pair (10M total)
    log:
        seqtk_err = "{DATA_PATH}/{experiment}/{sample}/logs/seqtk_subsampling.log.txt"
    threads: 8
    benchmark:
        "{DATA_PATH}/{experiment}/{sample}/logs/benchmark/seqtk_subsampling.txt"
    shell:
        """
        seqtk sample -s {params.seed} {input.r1_file} {params.reads} 2>{log.seqtk_err} | pigz --fast --processes {threads} >{output.r1_file_ss}
        seqtk sample -s {params.seed} {input.r2_file} {params.reads} 2>>{log.seqtk_err} | pigz --fast --processes {threads} >{output.r2_file_ss}
        """


### fastp
# We adapter trimming and poly-g filtering, but leave quality filtering / polyN filtering for later.
# NOTE: We also do read trimming here, 10-15 BP depending on EM- vs BS-seq
rule fastp:
    conda: "envs/env.yaml"
    input:
        md5_sums = ancient(rules.md5sum.output.validated),  # If md5 validation was ever done, skip it.
        r1_file = rules.md5sum.input.r1_file,
        r2_file = rules.md5sum.input.r2_file,
    output:
        fastp_pipe_output = temp(pipe("{DATA_PATH}/{experiment}/{sample}/fastp/interleaved.fa")),
        fastp_failed = "{DATA_PATH}/{experiment}/{sample}/fastp/failed.fa",  # Length < 15
        fastp_json = "{DATA_PATH}/{experiment}/{sample}/fastp/fastp.json",
        fastp_html = "{DATA_PATH}/{experiment}/{sample}/fastp/fastp.html"
    log:
        fastp_err = "{DATA_PATH}/{experiment}/{sample}/logs/fastp.log.txt"
    threads: 1  # One thread is never rate-limiting here
    params:
        # Inspired to do poly-g trimming by the NEB EM-seq pipeline:
        # https://github.com/nebiolabs/EM-seq/blob/master/em-seq.nf
        # TODO: make this selective for NextSeq/NovaSeq-only runs?
        # e.g.:
        #    inst_name=$(zcat -f '!{fq_set.insert_read1}' | head -n 1 | cut -f 1 -d ':' | sed 's/^@//')
        #    if [[ "${inst_name:0:2}" == 'A0' ]] || [[ "${inst_name:0:2}" == 'NS' ]] || \
        #       [[ "${inst_name:0:2}" == 'NB' ]] || [[ "${inst_name:0:2}" == 'VH' ]] ; then
        # trim_poly_g_flag = "--trim_poly_g" if {input...}.startswith(tuple(["@A0", "@NS", "@NB", "@VH"])) else "",
        trim_poly_g_flag = "--trim_poly_g",

        # Note: --overrepresentation_analysis was overwhelming with our huge sequencing data, disabled for now.

        # TODO: Trim specific to BS vs EM-seq input type
        trim_r1_5prime = "10",
        trim_r1_3prime = "10",
        trim_r2_5prime = "10",
        trim_r2_3prime = "10",

        minimum_length = 15,

        # TruSeq adapters (we could autodetect, but I prefer being explicit)   
        adapter_r1 = "AGATCGGAAGAGCACACGTCTGAACTCCAGTCA",
        adapter_r2 = "AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT",
    shell:
        """
        fastp --in1 {input.r1_file} --in2 {input.r2_file} \
        --trim_front1 {params.trim_r1_5prime} --trim_tail1 {params.trim_r1_3prime} \
        --trim_front2 {params.trim_r2_5prime} --trim_tail2 {params.trim_r2_3prime} \
        --length_required {params.minimum_length} --disable_quality_filtering {params.trim_poly_g_flag} \
        --adapter_sequence "{params.adapter_r1}" --adapter_sequence_r2 "{params.adapter_r2}" \
        --json "{output.fastp_json}" --html "{output.fastp_html}" \
        --thread {threads} --verbose --failed_out {output.fastp_failed} --stdout \
        2>{log.fastp_err} >{output.fastp_pipe_output}
        """

### bwa_meth
# This uses bwa mem, instead of bwa-mem2
# NOTE: You could also run bwa mem2. I don't here because of a compile-time bug:
# see https://github.com/bwa-mem2/bwa-mem2/issues/207
rule bwa_meth:
    conda: "envs/env.yaml"
    input:
        input_fasta = "{DATA_PATH}/{experiment}/{sample}/fastp/interleaved.fa",
        reference_genome = rules.mask_reference_fasta.output.masked_reference_genome,
        bwa_meth_indices = rules.bwa_meth_index.output
    output:
        temp(pipe("{DATA_PATH}/{experiment}/{sample}/samtools/raw.sam"))
    params:
        # TODO: Should we include the platform or barcode here? Does that have value for us?
        read_group = "@RG\\tID:{sample}\\tSM:{sample}"
    log:
        bwameth_err = "{DATA_PATH}/{experiment}/{sample}/logs/bwameth.log.txt"
    benchmark:
        "{DATA_PATH}/{experiment}/{sample}/logs/benchmark/bwameth.txt"
    threads: 40
    shell:
        "bwameth.py -p -t {threads} --read-group '{params.read_group}' --reference {input.reference_genome} {input.input_fasta} 2>{log.bwameth_err} >{output}"


### mark nonconverted reads
# This is a clever little program from NEB that marks non-converted reads:
# https://github.com/nebiolabs/mark-nonconverted-reads
# This sets XX:Z:UC (the X? Y? and Z? fields of SAM/BAM are user-reserved). We do *not* set the Vendor Failed bit.
# Note that  MethylDackel can independently identify poor conversion (set minConversionEfficiency).
rule mark_nonconverted:
    conda: "envs/env.yaml"
    input:
        aligned_sam = "{DATA_PATH}/{experiment}/{sample}/samtools/raw.sam",
        reference_genome = rules.mask_reference_fasta.output.masked_reference_genome
    output:
        temp(pipe("{DATA_PATH}/{experiment}/{sample}/samtools/nonconv-marked.sam"))
    log:
        nonconverted = "{DATA_PATH}/{experiment}/{sample}/logs/mark-nonconverted.log.txt"
    params:
        threshold = 3 # If 3 nonconverted Cs on a read, consider it nonconverted
    shell:
        "cat {input.aligned_sam} | mark-nonconverted-reads/mark-nonconverted-reads.py --c_count {params.threshold} --reference {input.reference_genome} 2>{log.nonconverted} >{output}"


### fixmate, sort, and markdup
## These were originally defined as named pipes between snakemake blocks,
## but I ran into a group bug that I couldn't work around: https://github.com/snakemake/snakemake/issues/1822 
## So now we're one large block shell command *shrug*.
# NOTE: This is the final step in the core alignment pipeline: markdup writes a (lightly) compressed .bam file and index.
rule samtools_fixmate_sort_markdup:
    conda: "envs/env.yaml"
    input:
        rules.mark_nonconverted.output
    output:
        bam = protected("{DATA_PATH}/{experiment}/{sample}/aligned.bam"),
        stats_file = "{DATA_PATH}/{experiment}/{sample}/samtools/markdup.txt"
    log:
        fixmate = "{DATA_PATH}/{experiment}/{sample}/logs/samtools-fixmate.log.txt",
        sort = "{DATA_PATH}/{experiment}/{sample}/logs/samtools-sort.log.txt",
        markdup = "{DATA_PATH}/{experiment}/{sample}/logs/samtools-markdup.log.txt"
    benchmark:
        "{DATA_PATH}/{experiment}/{sample}/logs/benchmark/samtools.txt"
    threads: 1
    shell:
        # fixmate is needed for markdup -- it adds ms and MC tags
        # fixmate: -u uncompressed output / -m add mate score tag
        # sort: coodinate sorted, uncompressed sam
        # TODO: Mark supplementary reads of duplicates as duplicates? (-S)
        # TODO: Specify index name here? Build an idx first? e.g. "-##idx##indexname"
        """
        cat {input} | samtools fixmate -u -m --output-fmt SAM --threads {threads} - - 2>{log.fixmate} | \
        samtools sort --output-fmt SAM --threads {threads} -T {resources.tmpdir} - 2>{log.sort} | \
        samtools markdup -f {output.stats_file} --output-fmt BAM --output-fmt-option level=1 --write-index --threads {threads} -T {resources.tmpdir} - {output.bam} 2> {log.markdup}
        """


###############
### Statistics and QC Graphs
###############

### samtools index
rule samtools_index:
    conda: "envs/env.yaml"
    input:
        rules.samtools_fixmate_sort_markdup.output.bam
    output:
        "{DATA_PATH}/{experiment}/{sample}/aligned.bam.bai"
    log:
        "{DATA_PATH}/{experiment}/{sample}/logs/samtools-index-bai.log.txt"
    shell:
        # Note: the markdup makes a .csi index, but more classically like .bai's
        "samtools index -b {input} >{log} 2>&1"

### samtools statistics
rule samtools_statistics:
    conda: "envs/env.yaml"
    input:
        bam = rules.samtools_fixmate_sort_markdup.output.bam,
        # Note: we can operate on the .csi index above, but I worry there's a race where
        # we spawn up idxstats but we only just started writing to a .bai
        index = rules.samtools_index.output
    output:
        flagstat = "{DATA_PATH}/{experiment}/{sample}/samtools/flagstat.txt",
        idxstats = "{DATA_PATH}/{experiment}/{sample}/samtools/idxstats.txt",
        stats = "{DATA_PATH}/{experiment}/{sample}/samtools/stats.txt",
    log:
        flagstat = "{DATA_PATH}/{experiment}/{sample}/logs/samtools-flagstat.log.txt",
        idxstats = "{DATA_PATH}/{experiment}/{sample}/logs/samtools-idxstats.log.txt",
        stats = "{DATA_PATH}/{experiment}/{sample}/logs/samtools-stats.log.txt"
    shell:
        """
        samtools flagstat {input.bam} 2>{log.flagstat} >{output.flagstat}
        samtools idxstats {input.bam} 2>{log.idxstats} >{output.idxstats}
        samtools stats {input.bam} 2>{log.stats} >{output.stats}
        """

### MethylDackel mbias
rule methyldackel_mbias_plots:
    conda: "envs/env.yaml"
    input:
        bam = rules.samtools_fixmate_sort_markdup.output.bam,
        reference = rules.mask_reference_fasta.output.masked_reference_genome,
        index = rules.samtools_index.output
    output:
        mbias_txt = "{DATA_PATH}/{experiment}/{sample}/methyldackel/mbias.txt",
        mbias_ot = "{DATA_PATH}/{experiment}/{sample}/methyldackel/mbias_OT.svg",
        mbias_ob = "{DATA_PATH}/{experiment}/{sample}/methyldackel/mbias_OB.svg"
    log:
        "{DATA_PATH}/{experiment}/{sample}/logs/methyldackel-mbias.log.txt"
    params:
        out_dir = "{DATA_PATH}/{experiment}/{sample}/methyldackel/mbias"
    threads: 1
    shell:
        "MethylDackel mbias -@ {threads} --txt {input.reference} {input.bam} {params.out_dir} 2>{log} >{output.mbias_txt}"

### FastQC stats
rule fastqc_bam:
    conda: "envs/env.yaml"
    input:
        rules.samtools_fixmate_sort_markdup.output.bam
    output:
        "{DATA_PATH}/{experiment}/{sample}/fastqc/aligned_fastqc.html"
    params:
        out_dir = "{DATA_PATH}/{experiment}/{sample}/fastqc/",
    log:
        "{DATA_PATH}/{experiment}/{sample}/logs/fastqc.log.txt"
    shell:
        """
        mkdir -p {params.out_dir}
        fastqc --outdir {params.out_dir} {input} >{log} 2>&1
        """
        
### goleft indexcov
rule goleft_indexcov:
    conda: "envs/env.yaml"
    input:
        bam = rules.samtools_fixmate_sort_markdup.output.bam,
        index = rules.samtools_index.output
    output:
        "{DATA_PATH}/{experiment}/{sample}/goleft/index.html"
    log:
        "{DATA_PATH}/{experiment}/{sample}/logs/goleft.log.txt"
    params:
        out_dir = "{DATA_PATH}/{experiment}/{sample}/goleft/"
    shell:
        # TODO: Run on all .bam files? Multiple from one project? All from a given sequencing batch?
        "goleft indexcov --directory {params.out_dir} {input.bam} >{log} 2>&1"


### DeepTools bias plots
# For GC bias, see https://deeptools.readthedocs.io/en/develop/content/tools/computeGCBias.html
# TODO: Implement this
rule deeptools_bias_plots:
    conda: "envs/env.yaml"
    input:
        # bam = rules.samtools_fixmate_sort_markdup.output.bam
    output:
        # plot = "{DATA_PATH}/{experiment}/{sample}/deeptools/gcbias.png",
        # stats = "{DATA_PATH}/{experiment}/{sample}/deeptools/gcbias.freq.txt"
    threads: 1
    shell:
        # TODO: Make the 2bit and compute genome size :/ -- lots of work for this to function
        "computeGCBias --verbose --numberOfProcessors {threads} --bamfile {input.bam} --effectiveGenomeSize XXXXXX --genome make.2bit --biasPlot {output.plot} --GCbiasFrequenciesFile {output.stats}"

### wgbs_tools
# Generate index
rule wgbs_tools_index:
    conda: "envs/env.yaml"
    input:
        rules.mask_reference_fasta.output.masked_reference_genome
    output:
        # TODO: where's the output?
    log:
        "{DATA_PATH}/{experiment}/{sample}/wgbs_tools/init_genome.log.txt"
    shell:
        # TODO: Fix this path
        "wgbstools init_genome GRCh38-DAC-U2AF1 --fasta_path {input} >{log} 2>&1"

### WGBS pat/beta file formats
rule wgbs_tools_pat_beta:
    conda: "envs/env.yaml"
    input:
        bam = rules.samtools_fixmate_sort_markdup.output.bam,
        index = rules.wgbs_tools_index.output
    output:
        "{DATA_PATH}/{experiment}/{sample}/wgbs_tools/{sample}.pat.gz",
        "{DATA_PATH}/{experiment}/{sample}/wgbs_tools/{sample}.beta"
        # TODO: What's the mbias output?
    log:
        "{DATA_PATH}/{experiment}/{sample}/logs/wgbs_tools-bam2pat.log.txt"
    params:
        out_dir = "{DATA_PATH}/{experiment}/{sample}/wgbs_tools/"
    threads: 1
    shell:
        # TODO: Fix this path
        "wgbstools bam2pat --genome GRCh38-DAC-U2AF1 --out_dir {params.out_dir} --mbias --verbose --threads {threads} {input.bam} >{log} 2>&1"

### multiqc
# Aggregate all our pretty data
# Note: This pan-dependency ensures all other jobs are run.
rule multiqc:
    conda: "envs/env.yaml"
    input:
        rules.samtools_statistics.output,
        rules.fastqc_bam.output,
        rules.methyldackel_mbias_plots.output,
        rules.goleft_indexcov.output
        # rules.wgbs_tools_pat_beta.output,
        # rules.deeptools_bias_plots.output
    output:
        "{DATA_PATH}/{experiment}/{sample}/multiqc/multiqc_report.html"
    params:
        in_dir = "{DATA_PATH}/{experiment}/{sample}/",
        out_dir = "{DATA_PATH}/{experiment}/{sample}/multiqc/"
    log:
        "{DATA_PATH}/{experiment}/{sample}/logs/multiqc.log.txt"
    shell:
        "multiqc --outdir {params.out_dir} --force --verbose --interactive --exclude snippy {params.in_dir} >{log} 2>&1"
